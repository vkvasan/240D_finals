{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('./gcn/gcn_model/')"
      ],
      "metadata": {
        "id": "l7wI5mygpBEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --nbits=4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiYIeJDCr-EZ",
        "outputId": "e23f5492-5eb2-4dc9-f9dc-98ce65700f6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n",
            "Epoch: 0001 loss_train: -0.1438 acc_train: 0.2143 loss_val: -0.1457 acc_val: 0.2033 time: 0.7669s\n",
            "Epoch: 0002 loss_train: -0.1503 acc_train: 0.2143 loss_val: -0.1528 acc_val: 0.2467 time: 0.0192s\n",
            "Epoch: 0003 loss_train: -0.1566 acc_train: 0.2714 loss_val: -0.1607 acc_val: 0.2567 time: 0.0192s\n",
            "Epoch: 0004 loss_train: -0.1644 acc_train: 0.3143 loss_val: -0.1689 acc_val: 0.2667 time: 0.0192s\n",
            "Epoch: 0005 loss_train: -0.1771 acc_train: 0.3429 loss_val: -0.1751 acc_val: 0.2933 time: 0.0192s\n",
            "Epoch: 0006 loss_train: -0.1803 acc_train: 0.3714 loss_val: -0.1860 acc_val: 0.3500 time: 0.0192s\n",
            "Epoch: 0007 loss_train: -0.1874 acc_train: 0.4214 loss_val: -0.1918 acc_val: 0.3733 time: 0.0191s\n",
            "Epoch: 0008 loss_train: -0.1996 acc_train: 0.4857 loss_val: -0.1969 acc_val: 0.4133 time: 0.0191s\n",
            "Epoch: 0009 loss_train: -0.1993 acc_train: 0.4786 loss_val: -0.2046 acc_val: 0.5233 time: 0.0191s\n",
            "Epoch: 0010 loss_train: -0.2009 acc_train: 0.5357 loss_val: -0.2116 acc_val: 0.5100 time: 0.0192s\n",
            "Epoch: 0011 loss_train: -0.2138 acc_train: 0.5643 loss_val: -0.2187 acc_val: 0.5867 time: 0.0192s\n",
            "Epoch: 0012 loss_train: -0.2151 acc_train: 0.5929 loss_val: -0.2217 acc_val: 0.6200 time: 0.0185s\n",
            "Epoch: 0013 loss_train: -0.2178 acc_train: 0.6429 loss_val: -0.2258 acc_val: 0.6633 time: 0.0161s\n",
            "Epoch: 0014 loss_train: -0.2274 acc_train: 0.6500 loss_val: -0.2271 acc_val: 0.6700 time: 0.0160s\n",
            "Epoch: 0015 loss_train: -0.2227 acc_train: 0.6857 loss_val: -0.2352 acc_val: 0.6867 time: 0.0161s\n",
            "Epoch: 0016 loss_train: -0.2337 acc_train: 0.7000 loss_val: -0.2366 acc_val: 0.6933 time: 0.0161s\n",
            "Epoch: 0017 loss_train: -0.2341 acc_train: 0.6929 loss_val: -0.2430 acc_val: 0.7067 time: 0.0160s\n",
            "Epoch: 0018 loss_train: -0.2411 acc_train: 0.6929 loss_val: -0.2442 acc_val: 0.7133 time: 0.0160s\n",
            "Epoch: 0019 loss_train: -0.2444 acc_train: 0.7143 loss_val: -0.2455 acc_val: 0.7233 time: 0.0160s\n",
            "Epoch: 0020 loss_train: -0.2463 acc_train: 0.7714 loss_val: -0.2472 acc_val: 0.7100 time: 0.0160s\n",
            "Epoch: 0021 loss_train: -0.2462 acc_train: 0.7643 loss_val: -0.2476 acc_val: 0.7200 time: 0.0160s\n",
            "Epoch: 0022 loss_train: -0.2482 acc_train: 0.7786 loss_val: -0.2501 acc_val: 0.7300 time: 0.0161s\n",
            "Epoch: 0023 loss_train: -0.2477 acc_train: 0.7857 loss_val: -0.2513 acc_val: 0.7400 time: 0.0162s\n",
            "Epoch: 0024 loss_train: -0.2516 acc_train: 0.8000 loss_val: -0.2572 acc_val: 0.7433 time: 0.0161s\n",
            "Epoch: 0025 loss_train: -0.2555 acc_train: 0.7929 loss_val: -0.2580 acc_val: 0.7433 time: 0.0161s\n",
            "Epoch: 0026 loss_train: -0.2555 acc_train: 0.7857 loss_val: -0.2605 acc_val: 0.7467 time: 0.0160s\n",
            "Epoch: 0027 loss_train: -0.2624 acc_train: 0.8214 loss_val: -0.2630 acc_val: 0.7533 time: 0.0160s\n",
            "Epoch: 0028 loss_train: -0.2597 acc_train: 0.8143 loss_val: -0.2631 acc_val: 0.7500 time: 0.0160s\n",
            "Epoch: 0029 loss_train: -0.2652 acc_train: 0.8143 loss_val: -0.2627 acc_val: 0.7567 time: 0.0160s\n",
            "Epoch: 0030 loss_train: -0.2666 acc_train: 0.8000 loss_val: -0.2708 acc_val: 0.7400 time: 0.0160s\n",
            "Epoch: 0031 loss_train: -0.2721 acc_train: 0.7929 loss_val: -0.2696 acc_val: 0.7467 time: 0.0160s\n",
            "Epoch: 0032 loss_train: -0.2792 acc_train: 0.8714 loss_val: -0.2726 acc_val: 0.7333 time: 0.0160s\n",
            "Epoch: 0033 loss_train: -0.2803 acc_train: 0.8786 loss_val: -0.2781 acc_val: 0.7367 time: 0.0160s\n",
            "Epoch: 0034 loss_train: -0.2811 acc_train: 0.8500 loss_val: -0.2771 acc_val: 0.7333 time: 0.0160s\n",
            "Epoch: 0035 loss_train: -0.2798 acc_train: 0.8714 loss_val: -0.2928 acc_val: 0.7500 time: 0.0160s\n",
            "Epoch: 0036 loss_train: -0.2965 acc_train: 0.8571 loss_val: -0.2918 acc_val: 0.7400 time: 0.0160s\n",
            "Epoch: 0037 loss_train: -0.2952 acc_train: 0.8429 loss_val: -0.2932 acc_val: 0.7367 time: 0.0160s\n",
            "Epoch: 0038 loss_train: -0.3009 acc_train: 0.8500 loss_val: -0.2946 acc_val: 0.7600 time: 0.0160s\n",
            "Epoch: 0039 loss_train: -0.3037 acc_train: 0.8714 loss_val: -0.2944 acc_val: 0.7633 time: 0.0160s\n",
            "Epoch: 0040 loss_train: -0.3031 acc_train: 0.8929 loss_val: -0.2958 acc_val: 0.7733 time: 0.0160s\n",
            "Epoch: 0041 loss_train: -0.3013 acc_train: 0.9000 loss_val: -0.2988 acc_val: 0.7933 time: 0.0160s\n",
            "Epoch: 0042 loss_train: -0.3127 acc_train: 0.9071 loss_val: -0.2987 acc_val: 0.7867 time: 0.0160s\n",
            "Epoch: 0043 loss_train: -0.3115 acc_train: 0.9214 loss_val: -0.3018 acc_val: 0.7900 time: 0.0160s\n",
            "Epoch: 0044 loss_train: -0.3059 acc_train: 0.8857 loss_val: -0.3069 acc_val: 0.7833 time: 0.0160s\n",
            "Epoch: 0045 loss_train: -0.3194 acc_train: 0.8786 loss_val: -0.3049 acc_val: 0.7867 time: 0.0160s\n",
            "Epoch: 0046 loss_train: -0.3110 acc_train: 0.9000 loss_val: -0.3093 acc_val: 0.7800 time: 0.0160s\n",
            "Epoch: 0047 loss_train: -0.3220 acc_train: 0.9214 loss_val: -0.3101 acc_val: 0.7733 time: 0.0160s\n",
            "Epoch: 0048 loss_train: -0.3205 acc_train: 0.9143 loss_val: -0.3172 acc_val: 0.7833 time: 0.0160s\n",
            "Epoch: 0049 loss_train: -0.3248 acc_train: 0.9143 loss_val: -0.3225 acc_val: 0.7733 time: 0.0160s\n",
            "Epoch: 0050 loss_train: -0.3409 acc_train: 0.9286 loss_val: -0.3232 acc_val: 0.7833 time: 0.0160s\n",
            "Epoch: 0051 loss_train: -0.3419 acc_train: 0.9429 loss_val: -0.3240 acc_val: 0.8033 time: 0.0161s\n",
            "Epoch: 0052 loss_train: -0.3413 acc_train: 0.9357 loss_val: -0.3273 acc_val: 0.7900 time: 0.0161s\n",
            "Epoch: 0053 loss_train: -0.3428 acc_train: 0.9071 loss_val: -0.3308 acc_val: 0.7833 time: 0.0160s\n",
            "Epoch: 0054 loss_train: -0.3418 acc_train: 0.9286 loss_val: -0.3290 acc_val: 0.7867 time: 0.0160s\n",
            "Epoch: 0055 loss_train: -0.3407 acc_train: 0.9500 loss_val: -0.3356 acc_val: 0.7967 time: 0.0160s\n",
            "Epoch: 0056 loss_train: -0.3526 acc_train: 0.9571 loss_val: -0.3362 acc_val: 0.8000 time: 0.0160s\n",
            "Epoch: 0057 loss_train: -0.3436 acc_train: 0.9571 loss_val: -0.3422 acc_val: 0.7867 time: 0.0160s\n",
            "Epoch: 0058 loss_train: -0.3523 acc_train: 0.9571 loss_val: -0.3485 acc_val: 0.8000 time: 0.0160s\n",
            "Epoch: 0059 loss_train: -0.3487 acc_train: 0.9143 loss_val: -0.3510 acc_val: 0.7833 time: 0.0160s\n",
            "Epoch: 0060 loss_train: -0.3598 acc_train: 0.9429 loss_val: -0.3517 acc_val: 0.7833 time: 0.0160s\n",
            "Epoch: 0061 loss_train: -0.3571 acc_train: 0.9071 loss_val: -0.3545 acc_val: 0.7933 time: 0.0160s\n",
            "Epoch: 0062 loss_train: -0.3680 acc_train: 0.9357 loss_val: -0.3538 acc_val: 0.7867 time: 0.0160s\n",
            "Epoch: 0063 loss_train: -0.3653 acc_train: 0.9286 loss_val: -0.3589 acc_val: 0.7967 time: 0.0160s\n",
            "Epoch: 0064 loss_train: -0.3762 acc_train: 0.9571 loss_val: -0.3619 acc_val: 0.7867 time: 0.0160s\n",
            "Epoch: 0065 loss_train: -0.3842 acc_train: 0.9643 loss_val: -0.3638 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0066 loss_train: -0.3767 acc_train: 0.9500 loss_val: -0.3576 acc_val: 0.8133 time: 0.0160s\n",
            "Epoch: 0067 loss_train: -0.3741 acc_train: 0.9571 loss_val: -0.3600 acc_val: 0.7967 time: 0.0160s\n",
            "Epoch: 0068 loss_train: -0.3843 acc_train: 0.9714 loss_val: -0.3643 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0069 loss_train: -0.3868 acc_train: 0.9786 loss_val: -0.3652 acc_val: 0.8000 time: 0.0160s\n",
            "Epoch: 0070 loss_train: -0.3726 acc_train: 0.9786 loss_val: -0.3544 acc_val: 0.8067 time: 0.0161s\n",
            "Epoch: 0071 loss_train: -0.3823 acc_train: 0.9643 loss_val: -0.3567 acc_val: 0.7967 time: 0.0160s\n",
            "Epoch: 0072 loss_train: -0.3780 acc_train: 0.9571 loss_val: -0.3593 acc_val: 0.7900 time: 0.0160s\n",
            "Epoch: 0073 loss_train: -0.3743 acc_train: 0.9571 loss_val: -0.3624 acc_val: 0.7867 time: 0.0160s\n",
            "Epoch: 0074 loss_train: -0.3873 acc_train: 0.9857 loss_val: -0.3651 acc_val: 0.7967 time: 0.0160s\n",
            "Epoch: 0075 loss_train: -0.3896 acc_train: 0.9714 loss_val: -0.3661 acc_val: 0.8000 time: 0.0160s\n",
            "Epoch: 0076 loss_train: -0.3948 acc_train: 0.9500 loss_val: -0.3675 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0077 loss_train: -0.3833 acc_train: 0.9286 loss_val: -0.3713 acc_val: 0.7967 time: 0.0160s\n",
            "Epoch: 0078 loss_train: -0.3962 acc_train: 0.9857 loss_val: -0.3746 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0079 loss_train: -0.3932 acc_train: 0.9714 loss_val: -0.3764 acc_val: 0.8000 time: 0.0160s\n",
            "Epoch: 0080 loss_train: -0.3982 acc_train: 0.9714 loss_val: -0.3789 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0081 loss_train: -0.4041 acc_train: 0.9786 loss_val: -0.3841 acc_val: 0.8167 time: 0.0160s\n",
            "Epoch: 0082 loss_train: -0.4035 acc_train: 0.9714 loss_val: -0.3872 acc_val: 0.8200 time: 0.0160s\n",
            "Epoch: 0083 loss_train: -0.4006 acc_train: 0.9714 loss_val: -0.3882 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0084 loss_train: -0.4023 acc_train: 0.9643 loss_val: -0.3901 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0085 loss_train: -0.4094 acc_train: 0.9643 loss_val: -0.3922 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0086 loss_train: -0.4107 acc_train: 0.9571 loss_val: -0.3943 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0087 loss_train: -0.4104 acc_train: 0.9786 loss_val: -0.3946 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0088 loss_train: -0.4142 acc_train: 0.9714 loss_val: -0.4000 acc_val: 0.8067 time: 0.0191s\n",
            "Epoch: 0089 loss_train: -0.4202 acc_train: 0.9643 loss_val: -0.4028 acc_val: 0.8000 time: 0.0160s\n",
            "Epoch: 0090 loss_train: -0.4211 acc_train: 0.9571 loss_val: -0.4048 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0091 loss_train: -0.4314 acc_train: 0.9786 loss_val: -0.4070 acc_val: 0.8133 time: 0.0160s\n",
            "Epoch: 0092 loss_train: -0.4302 acc_train: 0.9786 loss_val: -0.4070 acc_val: 0.8133 time: 0.0160s\n",
            "Epoch: 0093 loss_train: -0.4264 acc_train: 0.9857 loss_val: -0.4057 acc_val: 0.8133 time: 0.0160s\n",
            "Epoch: 0094 loss_train: -0.4256 acc_train: 0.9786 loss_val: -0.4073 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0095 loss_train: -0.4390 acc_train: 0.9786 loss_val: -0.4065 acc_val: 0.8167 time: 0.0160s\n",
            "Epoch: 0096 loss_train: -0.4343 acc_train: 0.9786 loss_val: -0.4083 acc_val: 0.8167 time: 0.0160s\n",
            "Epoch: 0097 loss_train: -0.4345 acc_train: 0.9714 loss_val: -0.4121 acc_val: 0.8167 time: 0.0160s\n",
            "Epoch: 0098 loss_train: -0.4480 acc_train: 0.9643 loss_val: -0.4154 acc_val: 0.8133 time: 0.0160s\n",
            "Epoch: 0099 loss_train: -0.4413 acc_train: 0.9643 loss_val: -0.4183 acc_val: 0.7967 time: 0.0160s\n",
            "Epoch: 0100 loss_train: -0.4433 acc_train: 0.9857 loss_val: -0.4189 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0101 loss_train: -0.4459 acc_train: 0.9643 loss_val: -0.4221 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0102 loss_train: -0.4488 acc_train: 0.9714 loss_val: -0.4209 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0103 loss_train: -0.4476 acc_train: 0.9786 loss_val: -0.4220 acc_val: 0.8133 time: 0.0160s\n",
            "Epoch: 0104 loss_train: -0.4657 acc_train: 0.9714 loss_val: -0.4234 acc_val: 0.8133 time: 0.0160s\n",
            "Epoch: 0105 loss_train: -0.4575 acc_train: 0.9857 loss_val: -0.4247 acc_val: 0.8167 time: 0.0160s\n",
            "Epoch: 0106 loss_train: -0.4502 acc_train: 0.9857 loss_val: -0.4256 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0107 loss_train: -0.4641 acc_train: 0.9571 loss_val: -0.4270 acc_val: 0.8133 time: 0.0160s\n",
            "Epoch: 0108 loss_train: -0.4675 acc_train: 0.9786 loss_val: -0.4269 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0109 loss_train: -0.4563 acc_train: 0.9571 loss_val: -0.4299 acc_val: 0.8133 time: 0.0160s\n",
            "Epoch: 0110 loss_train: -0.4559 acc_train: 0.9571 loss_val: -0.4329 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0111 loss_train: -0.4611 acc_train: 0.9786 loss_val: -0.4340 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0112 loss_train: -0.4639 acc_train: 0.9786 loss_val: -0.4361 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0113 loss_train: -0.4677 acc_train: 0.9786 loss_val: -0.4308 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0114 loss_train: -0.4731 acc_train: 0.9786 loss_val: -0.4346 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0115 loss_train: -0.4748 acc_train: 0.9714 loss_val: -0.4319 acc_val: 0.7900 time: 0.0160s\n",
            "Epoch: 0116 loss_train: -0.4787 acc_train: 0.9786 loss_val: -0.4360 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0117 loss_train: -0.4776 acc_train: 0.9857 loss_val: -0.4371 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0118 loss_train: -0.4757 acc_train: 0.9500 loss_val: -0.4384 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0119 loss_train: -0.4806 acc_train: 0.9643 loss_val: -0.4376 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0120 loss_train: -0.4774 acc_train: 0.9500 loss_val: -0.4376 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0121 loss_train: -0.4864 acc_train: 0.9857 loss_val: -0.4409 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0122 loss_train: -0.4794 acc_train: 0.9786 loss_val: -0.4440 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0123 loss_train: -0.4933 acc_train: 0.9786 loss_val: -0.4464 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0124 loss_train: -0.4948 acc_train: 0.9714 loss_val: -0.4472 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0125 loss_train: -0.4927 acc_train: 0.9571 loss_val: -0.4496 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0126 loss_train: -0.4788 acc_train: 0.9857 loss_val: -0.4505 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0127 loss_train: -0.4936 acc_train: 0.9714 loss_val: -0.4550 acc_val: 0.8133 time: 0.0160s\n",
            "Epoch: 0128 loss_train: -0.4941 acc_train: 0.9571 loss_val: -0.4546 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0129 loss_train: -0.4911 acc_train: 0.9786 loss_val: -0.4525 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0130 loss_train: -0.4998 acc_train: 0.9857 loss_val: -0.4530 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0131 loss_train: -0.4924 acc_train: 0.9714 loss_val: -0.4749 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0132 loss_train: -0.5152 acc_train: 0.9714 loss_val: -0.4742 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0133 loss_train: -0.5070 acc_train: 0.9786 loss_val: -0.4769 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0134 loss_train: -0.5159 acc_train: 0.9714 loss_val: -0.4778 acc_val: 0.8133 time: 0.0160s\n",
            "Epoch: 0135 loss_train: -0.5231 acc_train: 0.9786 loss_val: -0.4795 acc_val: 0.8133 time: 0.0160s\n",
            "Epoch: 0136 loss_train: -0.5204 acc_train: 0.9714 loss_val: -0.4804 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0137 loss_train: -0.5280 acc_train: 0.9643 loss_val: -0.4818 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0138 loss_train: -0.5359 acc_train: 0.9857 loss_val: -0.4837 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0139 loss_train: -0.5096 acc_train: 0.9571 loss_val: -0.4844 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0140 loss_train: -0.5309 acc_train: 0.9643 loss_val: -0.4848 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0141 loss_train: -0.5352 acc_train: 0.9571 loss_val: -0.4849 acc_val: 0.8000 time: 0.0160s\n",
            "Epoch: 0142 loss_train: -0.5291 acc_train: 0.9857 loss_val: -0.4887 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0143 loss_train: -0.5305 acc_train: 0.9786 loss_val: -0.4842 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0144 loss_train: -0.5376 acc_train: 0.9786 loss_val: -0.4868 acc_val: 0.8100 time: 0.0161s\n",
            "Epoch: 0145 loss_train: -0.5440 acc_train: 0.9571 loss_val: -0.4882 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0146 loss_train: -0.5295 acc_train: 0.9571 loss_val: -0.4918 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0147 loss_train: -0.5428 acc_train: 0.9714 loss_val: -0.4949 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0148 loss_train: -0.5440 acc_train: 0.9714 loss_val: -0.4941 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0149 loss_train: -0.5321 acc_train: 0.9714 loss_val: -0.4963 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0150 loss_train: -0.5455 acc_train: 0.9643 loss_val: -0.4972 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0151 loss_train: -0.5472 acc_train: 0.9643 loss_val: -0.4969 acc_val: 0.8133 time: 0.0161s\n",
            "Epoch: 0152 loss_train: -0.5487 acc_train: 0.9714 loss_val: -0.4983 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0153 loss_train: -0.5494 acc_train: 0.9714 loss_val: -0.4993 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0154 loss_train: -0.5517 acc_train: 0.9571 loss_val: -0.4997 acc_val: 0.7967 time: 0.0160s\n",
            "Epoch: 0155 loss_train: -0.5517 acc_train: 0.9786 loss_val: -0.5007 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0156 loss_train: -0.5578 acc_train: 0.9500 loss_val: -0.5013 acc_val: 0.8000 time: 0.0161s\n",
            "Epoch: 0157 loss_train: -0.5647 acc_train: 0.9786 loss_val: -0.5025 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0158 loss_train: -0.5664 acc_train: 0.9857 loss_val: -0.5032 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0159 loss_train: -0.5528 acc_train: 0.9786 loss_val: -0.5210 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0160 loss_train: -0.5787 acc_train: 0.9714 loss_val: -0.5205 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0161 loss_train: -0.5801 acc_train: 0.9786 loss_val: -0.5189 acc_val: 0.8133 time: 0.0160s\n",
            "Epoch: 0162 loss_train: -0.5941 acc_train: 0.9857 loss_val: -0.5164 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0163 loss_train: -0.5846 acc_train: 0.9786 loss_val: -0.5189 acc_val: 0.8167 time: 0.0160s\n",
            "Epoch: 0164 loss_train: -0.5892 acc_train: 0.9643 loss_val: -0.5189 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0165 loss_train: -0.5777 acc_train: 0.9714 loss_val: -0.5214 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0166 loss_train: -0.5990 acc_train: 0.9643 loss_val: -0.5236 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0167 loss_train: -0.5983 acc_train: 0.9714 loss_val: -0.5233 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0168 loss_train: -0.5954 acc_train: 0.9714 loss_val: -0.5251 acc_val: 0.8000 time: 0.0160s\n",
            "Epoch: 0169 loss_train: -0.5903 acc_train: 0.9571 loss_val: -0.5256 acc_val: 0.7933 time: 0.0160s\n",
            "Epoch: 0170 loss_train: -0.5979 acc_train: 0.9786 loss_val: -0.5268 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0171 loss_train: -0.6066 acc_train: 0.9571 loss_val: -0.5282 acc_val: 0.8033 time: 0.0160s\n",
            "Epoch: 0172 loss_train: -0.5914 acc_train: 0.9786 loss_val: -0.5328 acc_val: 0.8100 time: 0.0204s\n",
            "Epoch: 0173 loss_train: -0.6175 acc_train: 0.9786 loss_val: -0.5332 acc_val: 0.8000 time: 0.0161s\n",
            "Epoch: 0174 loss_train: -0.5920 acc_train: 0.9714 loss_val: -0.5349 acc_val: 0.8100 time: 0.0161s\n",
            "Epoch: 0175 loss_train: -0.6051 acc_train: 0.9786 loss_val: -0.5323 acc_val: 0.8000 time: 0.0160s\n",
            "Epoch: 0176 loss_train: -0.6053 acc_train: 0.9786 loss_val: -0.5330 acc_val: 0.7933 time: 0.0160s\n",
            "Epoch: 0177 loss_train: -0.6047 acc_train: 0.9714 loss_val: -0.5342 acc_val: 0.7967 time: 0.0160s\n",
            "Epoch: 0178 loss_train: -0.6092 acc_train: 0.9857 loss_val: -0.5379 acc_val: 0.7967 time: 0.0160s\n",
            "Epoch: 0179 loss_train: -0.6031 acc_train: 0.9714 loss_val: -0.5388 acc_val: 0.7833 time: 0.0160s\n",
            "Epoch: 0180 loss_train: -0.6196 acc_train: 0.9857 loss_val: -0.5374 acc_val: 0.7867 time: 0.0160s\n",
            "Epoch: 0181 loss_train: -0.5962 acc_train: 0.9786 loss_val: -0.5366 acc_val: 0.7833 time: 0.0161s\n",
            "Epoch: 0182 loss_train: -0.5994 acc_train: 0.9714 loss_val: -0.5378 acc_val: 0.7967 time: 0.0160s\n",
            "Epoch: 0183 loss_train: -0.6154 acc_train: 0.9786 loss_val: -0.5388 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0184 loss_train: -0.6005 acc_train: 0.9714 loss_val: -0.5443 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0185 loss_train: -0.6213 acc_train: 0.9643 loss_val: -0.5463 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0186 loss_train: -0.6242 acc_train: 0.9714 loss_val: -0.5475 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0187 loss_train: -0.6154 acc_train: 0.9786 loss_val: -0.5490 acc_val: 0.8133 time: 0.0160s\n",
            "Epoch: 0188 loss_train: -0.6239 acc_train: 0.9643 loss_val: -0.5508 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0189 loss_train: -0.6327 acc_train: 0.9714 loss_val: -0.5516 acc_val: 0.7967 time: 0.0160s\n",
            "Epoch: 0190 loss_train: -0.6120 acc_train: 0.9786 loss_val: -0.5533 acc_val: 0.8133 time: 0.0160s\n",
            "Epoch: 0191 loss_train: -0.6226 acc_train: 0.9786 loss_val: -0.5520 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0192 loss_train: -0.6328 acc_train: 0.9786 loss_val: -0.5529 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0193 loss_train: -0.6207 acc_train: 0.9857 loss_val: -0.5543 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0194 loss_train: -0.6175 acc_train: 0.9714 loss_val: -0.5559 acc_val: 0.8100 time: 0.0160s\n",
            "Epoch: 0195 loss_train: -0.6382 acc_train: 0.9786 loss_val: -0.5556 acc_val: 0.8000 time: 0.0160s\n",
            "Epoch: 0196 loss_train: -0.6365 acc_train: 0.9643 loss_val: -0.5554 acc_val: 0.8067 time: 0.0160s\n",
            "Epoch: 0197 loss_train: -0.6162 acc_train: 0.9714 loss_val: -0.5576 acc_val: 0.8100 time: 0.0162s\n",
            "Epoch: 0198 loss_train: -0.6384 acc_train: 0.9643 loss_val: -0.5582 acc_val: 0.8133 time: 0.0160s\n",
            "Epoch: 0199 loss_train: -0.6416 acc_train: 0.9857 loss_val: -0.5591 acc_val: 0.8167 time: 0.0160s\n",
            "Epoch: 0200 loss_train: -0.6494 acc_train: 0.9786 loss_val: -0.5584 acc_val: 0.8167 time: 0.0160s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 4.0237s\n",
            "Test set results: loss= -0.5432 accuracy= 0.8050\n"
          ]
        }
      ]
    }
  ]
}